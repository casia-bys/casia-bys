{
  "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
  "authors": [
    "Yecheng Jason Ma",
    "William Liang",
    "Guanzhi Wang",
    "De-An Huang",
    "Osbert Bastani",
    "Dinesh Jayaraman",
    "Yuke Zhu",
    "Linxi Fan",
    "Anima Anandkumar"
  ],
  "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
  "published": "2023-10-19",
  "arxiv_id": "2310.12931",
  "url": "https://arxiv.org/abs/2310.12931",
  "analysis": {
    "基本信息": {
      "论文标题": "Eureka: Human-Level Reward Design via Coding Large Language Models",
      "arXiv ID": "2310.12931",
      "关键词": []
    },
    "raw": "{\n  \"基本信息\": {\n    \"论文标题\": \"EUREKA: Human-Level Reward Design via Coding Large Language Models\",\n    \"arXiv ID\": \"2310.12931\",\n    \"作者\": [\n      \"Yecheng Jason Ma\",\n      \"William Liang\",\n      \"Guanzhi Wang\"\n    ],\n    \"所属机构\": \"NVIDIA, University of Pennsylvania, California Institute of Technology, University of Texas at Austin\",\n    \"关键词\": [\n      \"强化学习\",\n      \"奖励函数设计\",\n      \"大语言模型\",\n      \"机器人技能学习\",\n      \"进化算法\"\n    ]\n  },\n  \"0_摘要翻译\": {\n    \"背景\": \"大语言模型在作为高级语义规划器用于序贯决策任务方面表现出色。然而，利用它们学习复杂的低层操作任务，如巧妙的转笔技术，仍然是一个未解决的问题。我们弥合了这一根本差距，提出了EUREKA，一个由LLM驱动的类人水平奖励设计算法。\",\n    \"问题\": \"现有方法无法有效利用LLM解决复杂低层操作任务，特别是需要精确奖励信号塑造的复杂技能学习问题。\",\n    \"方法\": \"EUREKA利用GPT-4等前沿LLM的卓越零样本生成、代码编写和上下文内改进能力，对奖励代码进行进化优化。由此产生的奖励可用于通过强化学习获取复杂技能。无需任何任务特定的提示或预定义奖励模板，EUREKA生成的奖励函数优于专家人工设计的奖励。\",\n    \"结果\": \"在包含10种不同机器人形态的29个开源RL环境套件中，EUREKA在83%的任务上超越人类专家，平均归一化改进达52%。此外，EUREKA还实现了无梯度的上下文内强化学习从人类反馈(RLHF)方法，首次在模拟环境中展示机器人手完成快速转笔技巧。\"\n  },\n  \"1_问题背景\": {\n    \"研究问题\": \"如何利用大语言模型自动设计有效的奖励函数以实现复杂机器人操作技能的学习？\",\n    \"现有挑战\": [\n      \"挑战1：现有LLM在机器人领域的应用主要停留在高级语义规划层，缺乏有效利用其代码编写能力处理低层控制任务的方案\",\n      \"挑战2：复杂操作任务（如转笔）需要精确的奖励函数设计，但人工设计奖励函数耗时且易次优，92%的研究者依赖试错法\"\n    ],\n    \"研究动机\": \"奖励函数是强化学习的核心，但手动设计耗时且易出错。需要一个通用算法利用LLM的代码生成能力自动创建高质量奖励函数，特别是针对需要精细操作技能的复杂机器人任务。\",\n    \"gap分析\": \"现有方法如L2R依赖任务特定提示和模板，EUREKA则完全无需人工模板，能够通过上下文学习自主生成和优化奖励函数，突破了这一限制。\"\n  },\n  \"2_方法核心\": {\n    \"方法概述\": \"EUREKA是一个利用大语言模型进行奖励函数编程的进化式算法。它通过三步流程工作：1）直接将原始环境代码作为上下文喂给LLM零样本生成初始奖励函数；2）在LLM上下文内进行进化搜索，迭代生成和评估多个奖励候选；3）通过奖励反思机制根据评估结果有针对性地改进最佳奖励。整个过程无需任何任务特定提示或预定义模板。\",\n    \"整体架构\": \"输入：任务描述文本 + 环境源代码（剔除奖励部分）→ 编码LLM（如GPT-4）→ 生成多个奖励候选代码 → 并行评估奖励函数 → 奖励反思模块分析评估结果 → 更新提示 → 迭代优化 → 输出最优奖励函数\",\n    \"核心创新\": [\n      \"创新点1：环境作为上下文：直接输入原始环境代码给LLM，无需API或结构化接口\",\n      \"创新点2：进化式上下文内优化：使用LLM上下文窗口作为“进化记忆”，迭代生成候选奖励并基于评估结果改进\",\n      \"创新点3：奖励反思机制：通过文本化评估统计信息（如性能指标）自动生成改进建议\"\n    ]\n  },\n  \"3_具体方法（关键）\": {\n    \"模块1\": {\n      \"名称\": \"环境代码解析器\",\n      \"作用\": \"从原始环境代码中提取可用的状态/动作变量信息\",\n      \"输入输出\": \"输入：环境源代码文件；输出：环境变量定义片段和提示\",\n      \"实现细节\": \"使用自动脚本提取暴露环境状态和动作的代码段（如`self.state`, `self.action`），确保不泄露模拟器",
    "解析状态": "API返回格式异常"
  },
  "analyzed_at": "2026-01-22T21:53:16.087461"
}