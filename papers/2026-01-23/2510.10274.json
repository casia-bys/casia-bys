{
  "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
  "authors": [
    "Jinliang Zheng",
    "Jianxiong Li",
    "Zhihao Wang",
    "Dongxiu Liu",
    "Xirui Kang",
    "Yuchun Feng",
    "Yinan Zheng",
    "Jiayin Zou",
    "Yilun Chen",
    "Jia Zeng",
    "Ya-Qin Zhang",
    "Jiangmiao Pang",
    "Jingjing Liu",
    "Tai Wang",
    "Xianyuan Zhan"
  ],
  "abstract": "Successful generalist Vision-Language-Action (VLA) models rely on effective training across diverse robotic platforms with large-scale, cross-embodiment, heterogeneous datasets. To facilitate and leverage the heterogeneity in rich, diverse robotic data sources, we propose a novel Soft Prompt approach with minimally added parameters, by infusing prompt learning concepts into cross-embodiment robot learning and introducing separate sets of learnable embeddings for each distinct data source. These embeddings serve as embodiment-specific prompts, which in unity empower VLA models with effective exploitation of varying cross-embodiment features. Our new X-VLA, a neat flow-matching-based VLA architecture, relies exclusively on soft-prompted standard Transformer encoders, enjoying both scalability and simplicity. Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep of benchmarks, demonstrating superior results on a wide axes of capabilities, from flexible dexterity to quick adaptation across embodiments, environments, and tasks. Website: https://thu-air-dream.github.io/X-VLA/",
  "published": "2025-10-11",
  "arxiv_id": "2510.10274",
  "url": "https://arxiv.org/abs/2510.10274",
  "analysis": {
    "基本信息": {
      "论文标题": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
      "arXiv ID": "2510.10274",
      "作者": [
        "Jinliang Zheng",
        "Jianxiong Li",
        "Zhihao Wang"
      ],
      "所属机构": "Institute for AI Industry Research (AIR), Tsinghua University, Shanghai AI Lab, Peking University",
      "关键词": [
        "Vision-Language-Action",
        "Soft Prompt",
        "Cross-Embodiment",
        "Flow Matching",
        "Transformer"
      ]
    },
    "0_摘要翻译": {
      "背景": "成功的通用Vision-Language-Action (VLA)模型依赖于在具有大规模、跨体现、异构数据集的多样化机器人平台上的有效训练。为了在丰富多样的机器人数据源中促进和利用异构性，我们提出了一种新颖的软提示方法，只需添加最少的参数，通过将提示学习概念注入跨体现机器人学习，并为每个不同的数据源引入单独的可学习嵌入集。",
      "问题": "VLA模型面临来自硬件配置到数据收集策略的重大异构性挑战。这种异构性不仅体现在体现特定的动作空间中，还体现在诸如相机设置、视觉领域和任务分布等设置变化上。这些各种多样性维度导致跨体现的严重分布转移以及显著的语义错位，混淆模型并最终导致不满意的预训练和适应性能。",
      "方法": "这些嵌入作为体现特定的提示，共同使VLA模型能够有效利用不同的跨体现特征。我们新颖的X-VLA是一个基于流匹配的简洁VLA架构，完全依赖于软提示的标准Transformer编码器，享受可扩展性和简洁性。通过为每个数据源分配一组可学习嵌入作为软提示，这些嵌入为早期特征融合阶段提供了异构性感知指导，使VLA模型具有增强的容量来利用和巩固跨体现变化。",
      "结果": "在6个仿真环境和3个真实机器人上进行评估，我们0.9B实例化的X-VLA-0.9B同时在各种基准测试中实现了SOTA性能，在从灵活灵巧性到跨体现、环境和任务的快速适应能力的广泛能力轴上展示了卓越的结果。"
    },
    "1_问题背景": {
      "研究问题": "如何构建能够有效处理跨体现、异构机器人数据的大规模VLA模型",
      "现有挑战": [
        "挑战1：硬件配置的异构性导致分布转移和语义错位，从单臂到双臂设置，从相机设置到视觉领域的多样性维度混淆模型",
        "挑战2：缺乏标准化的数据收集协议和硬件平台间的不一致性，使得跨体现知识蒸馏成为未解决的问题"
      ],
      "研究动机": "构建能够灵活遵循任意人类指令并在多样化环境和不同体现中灵巧操作的自主代理是机器人社区的核心愿望，现有的VLA方法主要关注体现特定的动作解码头，而其他关键异构性来源不可避免地被忽视",
      "gap分析": "现有方法主要通过分配不同的动作解码头来适应体现特定的动作空间，但忽略了其他重要的异构性维度，如相机设置、视觉域和任务分布的变化，且无法有效整合异构的多域数据集"
    },
    "2_方法核心": {
      "方法概述": "X-VLA引入了一种软提示机制，为每个数据源分配一组可学习嵌入作为体现特定提示。这些嵌入在早期特征融合阶段提供异构性感知指导，使VLA模型能够利用和巩固跨体现变化。架构完全基于软提示的标准Transformer编码器，采用流匹配进行动作生成，支持可扩展的多模态特征融合。",
      "整体架构": "X-VLA采用基于流的策略生成架构，通过软提示机制整合多模态输入(视觉、语言、本体感受)特征。核心是软提示的自注意力Transformer块堆叠，用于融合多视图图像、语言提示和本体感受特征，实现精确动作生成。",
      "核心创新": [
        "创新点1：引入软提示机制，将不同硬件配置和数据类型建模为任务特定特征，通过可学习嵌入捕获异构性",
        "创新点2：采用流匹配策略而非直接动作预测，学习速度场从噪声样本传输到目标动作块",
        "创新点3：设计参数高效的两阶段训练流程，预训练阶段学习体现无关策略，适应阶段仅优化新软提示"
      ]
    },
    "3_具体方法（关键）": {
      "模块1": {
        "名称": "Soft Prompt机制",
        "作用": "处理跨体现异构性，为不同数据源提供异构性感知指导",
        "输入输出": "输入：数据集ID；输出：对应的可学习嵌入向量",
        "实现细节": "为每个数据源/硬件配置分配独立的一组可学习嵌入，这些嵌入在特征融合早期被查询并注入到Transformer块中，充当体现特定的软提示",
        "关键参数": "每个数据源配置一套完整的软提示嵌入，具体维度与Transformer隐藏层匹配"
      },
      "模块2": {
        "名称": "多模态编码器",
        "作用": "编码视觉、语言和本体感受信息",
        "输入输出": "输入：多视图图像、语言指令、本体感受状态；输出：统一的多模态特征表示",
        "实现细节": "使用预训练的ViT作为视觉编码器，大语言模型处理语言输入，简单的MLP编码本体感受特征，所有特征投影到统一维度后进行融合",
        "关键参数": "视觉特征维度：768（ViT-Base），语言特征维度：与VLM对齐的维度，本体感受特征：直接映射到统一维度"
      },
      "模块3": {
        "名称": "流匹配动作生成器",
        "作用": "通过学习速度场生成精确动作",
        "输入输出": "输入：编码的多模态特征、时间步t；输出：动作块的预测速度",
        "实现细节": "采用流匹配损失训练速度场v_θ(A_t, o, t)，通过线性插值路径对齐速度，使用Euler-Maruyama方法进行推理时ODE求解",
        "关键参数": "流匹配时间步t~U(0,1)，动作块大小T，推理使用欧拉方法，时间步长Δt"
      },
      "模块4": {
        "名称": "参数高效适应模块",
        "作用": "在新域上快速适应，保持预训练知识",
        "输入输出": "输入：目标域的软提示配置；输出：适配后的策略",
        "实现细节": "冻结预训练骨干网络，仅优化新域的软提示嵌入，配合LoRA等参数高效微调技术更新少量参数",
        "关键参数": "仅优化1%参数（约9M），新域软提示的嵌入维度与预训练保持一致"
      }
    },
    "4_训练策略": {
      "训练流程": "分为两阶段：阶段一预训练在290K异构数据混合上训练（包括Droid、Robomind、Agibot），涵盖7个平台5种机器人臂类型；阶段二域适应仅训练新域的软提示嵌入，配合LoRA进行参数高效微调",
      "损失函数": [
        {
          "名称": "L_FM^BC",
          "公式": "期望值E[t~U(0,1), (o,A)~D] [ ||v_θ(A_t, o, t) - (A-A_0)||^2 ]",
          "作用": "训练流匹配速度场，使预测速度沿最优传输路径对齐"
        },
        {
          "名称": "LoRA损失",
          "公式": "冻结主干，优化低秩适配器参数",
          "作用": "参数高效微调，仅更新少量参数适应新域"
        }
      ],
      "优化器": "AdamW优化器，学习率需在论文中找到具体数值",
      "学习率调度": "预训练阶段使用余弦退火调度，适应阶段使用较低学习率",
      "正则化": "使用Dropout、权重衰减等标准正则化技术",
      "数据增强": "使用随机裁剪、颜色抖动等视觉数据增强方法"
    },
    "5_特征设计": {
      "输入特征": "多视图RGB图像序列、语言指令文本、本体感受状态（关节角度、力矩等）、动作序列",
      "特征提取": "视觉：预训练ViT编码器；语言：大语言模型编码器；本体感受：简单MLP；动作：流匹配的噪声初始化",
      "特征融合": "所有模态特征投影到统一维度后，通过软提示增强的Transformer自注意力机制进行融合，软提示作为key-value对注入注意力计算",
      "特征维度": "统一隐藏维度d_model，视觉特征768维，语言特征与VLM对齐，本体感受特征通过线性层映射到d_model维"
    },
    "6_实验": {
      "数据集": {
        "训练集": "预训练：290Kepisode混合数据，包含Droid、Robomind、Agibot数据集，涵盖7个平台5种机器人臂类型；适应阶段：各目标域的特定数据集",
        "测试集": "6个仿真基准（Libero、Calvin、VLABench等）+ 1个自主驾驶基准 + 3个真实机器人平台",
        "数据来源": "多源异构机器人数据集，经过标准化预处理和软提示标注"
      },
      "评估指标": "成功率（Success Rate）、任务完成时间、精确度指标等",
      "实现细节": {
        "硬件": "具体硬件配置需从论文正文获取",
        "框架": "深度学习框架（如PyTorch）",
        "batch_size": "具体批量大小需从论文中获取",
        "训练时间": "预训练阶段具体时长未在摘要中明确",
        "参数量": "X-VLA-0.9B模型参数量为0.9B，LoRA适配器仅9M参数"
      },
      "对比方法": [
        "基线方法1：π0模型，3B参数量的VLA基线",
        "基线方法2：其他现有SOTA VLA方法",
        "基线方法3：不同异构性处理策略（如领域特定动作投影、HPT风格投影等）"
      ],
      "实验设置": "跨域适应实验、仿真基准测试、真实机器人验证、消融实验（软提示机制贡献分析）"
    },
    "7_结果分析": {
      "主结果": "X-VLA-0.9B在6个仿真基准和3个真实机器人上达到SOTA性能；在Libero基准达到93%成功率，在Simpler-WidowX达到54%成功率；相比π0方法（3B参数），仅用9M参数达到相当性能",
      "消融实验": "软提示机制显著提升跨体现适应性能；流匹配优于直接动作预测；参数高效适应策略有效保持预训练知识",
      "定性结果": "展示了强大的灵巧性操作能力，在真实世界中能够用1200个演示完成复杂的布料折叠任务，平均每2分钟折叠1块布料",
      "失败案例": "论文未明确提及失败案例，主要强调方法的优势和成功结果"
    },
    "8_借鉴句子": [
      "这些嵌入作为体现特定的提示，共同使VLA模型能够有效利用不同的跨体现特征。",
      "通过软提示，X-VLA可以被显式学习的个体硬件配置引导，以适应各种系统和数据结构。",
      "X-VLA允许可扩展的VLA训练，通过简单堆叠标准Transformer编码器进行多模态特征融合和精确动作生成。"
    ],
    "9_专业术语": [
      {
        "术语": "Vision-Language-Action (VLA)",
        "中文": "视觉-语言-动作模型",
        "解释": "统一多模态理解和动作生成的机器人控制模型"
      },
      {
        "术语": "Soft Prompt",
        "中文": "软提示",
        "解释": "可学习的嵌入向量，用以为模型提供域特定指导而非硬编码的离散提示"
      },
      {
        "术语": "Cross-Embodiment",
        "中文": "跨体现",
        "解释": "模型能够适应和操作具有不同硬件配置和物理形态的机器人系统"
      },
      {
        "术语": "Flow Matching",
        "中文": "流匹配",
        "解释": "通过学习速度场将噪声样本传输到目标数据的生成建模方法"
      },
      {
        "术语": "Heterogeneous Data",
        "中文": "异构数据",
        "解释": "来源于不同机器人平台、具有不同结构和特征分布的机器人数据集"
      }
    ],
    "10_图表分析": [
      {
        "图表": "图1：X-VLA架构概览",
        "内容": "展示了X-VLA的完整架构，包括视觉编码器、软提示机制、多模态融合的Transformer堆叠，以及流匹配动作生成器。右侧展示了不同硬件平台的软提示库配置，包括Agibot、Agilex、Franka、UR5等多个机器人平台的软提示配置。",
        "设计亮点": "清晰展示了从多模态输入到动作输出的完整流程，特别突出了软提示机制如何处理跨体现异构性，架构图简洁明了地传达了方法的核心思想。",
        "可借鉴点": "使用多列布局展示架构流程，用不同颜色区分不同组件，右侧的软提示库配置图直观展示了方法的实际应用场景。",
        "关键发现": "软提示机制是连接异构输入和统一架构的关键桥梁，Transformer堆叠提供了可扩展的架构基础。"
      },
      {
        "图表": "图2：方法对比图",
        "内容": "对比了四种处理跨体现异构性的方法：(a)领域特定动作投影，(b)HPT风格投影，(c)语言提示，(d)软提示方法。每种方法都显示了数据集输入到VLA模型再到动作输出的完整流程，以及它们如何查询数据集ID。",
        "设计亮点": "并列对比四种方法，清晰展示了各自优劣势，软提示方法在简洁性和有效性间取得平衡。",
        "可借鉴点": "使用相同的视觉语言描述不同方法的共同结构，突出差异点，便于读者快速理解各种方法的本质区别。",
        "关键发现": "软提示方法相比其他方法更加简洁，避免了复杂的动作投影或语言描述，同时有效处理了异构性。"
      },
      {
        "图表": "性能对比图表",
        "内容": "展示了X-VLA-0.9B在Libero基准上93%成功率和Simpler-WidowX上54%成功率的对比结果，以及与π0等方法的参数效率对比。",
        "设计亮点": "数值对比清晰直观，突出了方法在性能和效率方面的优势。",
        "可借鉴点": "使用柱状图或表格形式展示关键性能指标，配合参数效率对比图。",
        "关键发现": "X-VLA在达到SOTA性能的同时，显著降低了参数需求，展现了方法的实用性。"
      }
    ],
    "11_论文总结": {
      "核心贡献": [
        "提出了软提示机制有效处理跨体现异构性问题，为每个数据源引入可学习嵌入",
        "设计了基于流匹配的简洁VLA架构，完全依赖标准Transformer编码器实现可扩展性",
        "实现了参数高效的两阶段训练流程，在保持性能的同时显著降低适应成本"
      ],
      "方法优势": "相比现有方法具有更好的可扩展性、简洁性和参数效率，能够有效整合异构数据并快速适应新域",
      "局限性": "论文主要展示了成功结果，对失败案例和局限性讨论较少；软提示的具体实现细节和超参数设置需要进一步明确",
      "应用场景": "适用于多机器人协作、跨域机器人部署、通用机器人助手等需要处理异构硬件平台的场景",
      "扩展方向": "可以探索更大规模的预训练、更复杂的软提示设计、以及软提示在更广泛机器人任务中的应用"
    }
  },
  "analyzed_at": "2026-01-23T22:56:16.779233"
}